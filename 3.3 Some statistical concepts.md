# 3.3 Statistical concepts: mean, variance and standard deviation

The following are some statistical concepts that we will use during this unit. You don't need to learn the formulas below, but you should understand what the concepts are trying to measure. That conceptual knowledge will help you through later material in this course.

## The mean

The (arithmetic) mean of a data set is the sum of the values in the data set divided by the number of values. Colloquially, this is often called the "average", although there are many different concepts that might be called an average.

When doing experiments, there are two mean measurements of interest.

The first is the underlying mean of the process or population that you are trying to measure (often called the population mean). For example, what is the mean effect of sending a reminder letter to taxpayers? The population mean is usually denoted by $\mu$. We rarely get to know the population mean.

The second is the sample mean. The sample mean is the mean effect you actually measure in the experiment. (More precisely in this experiment, you will often have two sample means and be interested in the difference between them.) You measure the sample mean in an attempt to estimate the population mean. The sample mean is usually denoted by $\bar{x}$.

Mathematically, the sample mean $\bar{x}$ is calculated as follows:

$\bar{x}=\frac{x_1+x_2+x_3+...+x_n}{n}$

Where $x_1, x_2, ...., x_n$ are each of the $n$ observations that form the sample.

A related concept to the mean is that of the population and sample *proportion*, which are used to describe the percentage of a population or sample that possess a certain characteristic. The sample proportion is defined as the number of with the characteristic (*x*) divided by the size of the sample.

$\hat{p}=\frac{x}{n}$

## Variance and standard deviation

The variance is a measure of how much the values in a data set or a population vary from the mean.

Like the mean, we can talk of population variance and sample variance. Population variance is the variance of the full data set. As for the population mean, we rarely get to observe the population variance by accessing data points for the full population. The population variance is usually denoted by $\sigma^2$.

The sample variance relates to the sample observed in the experiment. The sample variance provides an estimate of population variance. The sample variance is usually denoted by $s^2$.

The sample variance is calculated as the mean squared difference of the values from the mean as follows:

$s^2=\frac{(x_1-\bar{x})^2+(x_2-\bar{x})^2+(x_3-\bar{x})^2+...+(x_n-\bar{x})^2}{n-1}$

(For those interested in technicalities, population variance is calculated by dividing by $n$ rather than $n-1$, with the different denominator for sample variance designed to remove bias in the estimate of the variance from the sample.)

The standard deviation is the square root of the variance. Standard deviation provides a more intuitive measure of variation as it is in the same scale as the distribution. For example, if the mean is in \$ or metres, the standard deviation is also measured in \$ or metres.

$s=\sqrt{s^2}$

Where we are measuring proportions, the sample standard deviation is a function of the sample proportion and sample size.

$s=\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$

## The standard error of the mean

The sample standard error is the estimated standard deviation of the sample mean.

To understand what the standard error is, let us return to our example of sending a letter to taxpayers to get them to submit their tax returns on time. Suppose you run the trial and 76% of people in the control group submit on time, compared to 79% in the intervention group. There is a 3 percentage point difference between the two groups. This is your difference between the two sample means.

When you measure a mean effect, it will vary by chance each time you measure it. How much variation should you expect in your estimate? The standard error provides an estimate of that variation. If you were to take a series of samples from a population, and take the mean of each one, that data set of means would have a standard deviation itself. The standard error is an estimate of that standard deviation.

Importantly, the concept of standard error should be distinguished from the standard deviation of the sample or population. Standard error relates to the distribution of means that you would get through repeated sampling. The standard distribution of the sample relates to the distribution of the overall sample. They are related, however, and as you can see from the formula, are proportional.

Where we are deriving the standard error for a single mean, we calculate as follows:

$s_{\bar{x}}=\frac{s}{\sqrt{n}}$

The formula is somewhat more complicated for the standard error of the difference between two different means. Here it is defined as:

$s_{\bar{x}}=\sqrt{\frac{s^2_0}{\sqrt{n_0}}+\frac{s^2_1}{\sqrt{n_1}}}$

where $n_0$ and $n_1$ are the number of observations in our sample from each of the two populations (for instance, the number in each of our control and treatment groups), and $s_0$ and $s_1$ are the sample variance for each sample.

An important feature of the standard error is that it shrinks with sample size. (Can you see this in the formula?) Larger samples give you a more precise measure of the mean, although it shrinks with the square root of the sample size. You need to increase the sample size by a factor of four to halve the standard error.

## Confidence interval

A confidence interval gives us a range of values for our variable of interest, such as the mean effect size, that is consistent with the data. If you were to take many repeated samples, the 95% confidence interval for each sample would contain the true value 95% of the time.

The confidence interval is closely linked to the concept of standard error. Depending on the distribution of the variable of interest, the confidence interval is typically calculated as the estimate of the mean $\pm$ a set number of standard errors.
